check:
  benchmarks:
  - confidence: 0.8
    reason: Tokenization can help mitigate the risk of valid accounts being compromised
      and used for unauthorized access to sensitive data.
    unique_id: MITRE-ATT&CK-T1078
  - confidence: 0.9
    reason: Tokenization can help protect sensitive data stored in cloud storage objects
      from unauthorized access or data theft.
    unique_id: MITRE-ATT&CK-T1530
  category: encryption
  controls:
  - confidence: 0.9
    reason: This control requires the protection of information at rest, which can
      be achieved through tokenization techniques.
    unique_id: NIST-800-53-SC-28
  - confidence: 0.8
    reason: This control requires the handling of assets, which includes protecting
      sensitive data through techniques like tokenization.
    unique_id: ISO-27001-A.8.2.3
  literature: Tokenization is a data security technique that replaces sensitive data
    elements with non-sensitive placeholders called tokens. This process helps protect
    sensitive data by removing it from the operational environment and storing it
    in a secure token vault. Tokenization is an effective method for meeting data
    security and compliance requirements, as it reduces the risk of data breaches
    and unauthorized access to sensitive information. By implementing tokenization
    techniques, organizations can minimize the exposure of sensitive data, such as
    credit card numbers, social security numbers, and other personally identifiable
    information (PII), while still allowing authorized systems and applications to
    process and use the tokenized data. This check validates that tokenization techniques
    are implemented to protect sensitive data at rest and in transit.
  severity: high
  tags:
  - data_protection
  - compliance
  - pci_dss
  - gdpr
  - hipaa
  unique_id: owasp-latest-implement-tokenization-techniques
